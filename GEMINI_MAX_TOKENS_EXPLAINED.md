# Comportamento MAX_TOKENS do Gemini API

## O Problema

Durante os testes, observamos que prompts simples retornam `finishReason: MAX_TOKENS` mesmo ANTES de gerar qualquer texto, resultando em `content: {"role":"model"}` sem array `parts`.

## Por Que Acontece

### Limites da FREE Tier (Google AI Studio)
Conforme dashboard do projeto `psicologobelohorizonte`:
- **RPM (Requests Per Minute)**: 5 / 1.000 limite
- **TPM (Tokens Per Minute)**: 161 / 1.000.000 limite
- **RPD (Requests Per Day)**: 10 / 10.000 limite

### Rate Limiting "Soft"
Quando mÃºltiplas chamadas sÃ£o feitas rapidamente, o Gemini impÃµe um **soft throttling** invisÃ­vel:
1. Aceita a requisiÃ§Ã£o
2. Retorna imediatamente `MAX_TOKENS`
3. NÃ£o gera nenhum token (`parts: []`)
4. Isso protege a quota sem retornar erro HTTP

## EvidÃªncias dos Testes

### âœ… Prompts que FUNCIONAM (primeiro teste)
```typescript
"Liste 3 frutas"               â†’ STOP (sucesso)
"DÃª 3 nomes de pessoas"        â†’ STOP (sucesso)
"Liste 3 cores"                â†’ STOP (sucesso)
"DÃª 3 respostas curtas"        â†’ STOP (sucesso)
```

### âŒ Prompts que FALHAM
```typescript
"Liste 3 respostas para: X"    â†’ MAX_TOKENS
Qualquer prompt com "para:"     â†’ MAX_TOKENS
Testes subsequentes rÃ¡pidos     â†’ MAX_TOKENS
Mesmo prompts que funcionaram   â†’ MAX_TOKENS (em 2Âª chamada rÃ¡pida)
```

## PadrÃ£o Descoberto

1. **Primeira chamada**: Geralmente funciona âœ…
2. **Segunda chamada rÃ¡pida (<1s)**: MAX_TOKENS âŒ
3. **Depois de delay (2-3s)**: Funciona novamente âœ…
4. **Prompts com "para:" + contexto**: SEMPRE falham âŒ

## SoluÃ§Ã£o Implementada

### 1. Delay EstratÃ©gico (500ms)
```typescript
// Em server/routes/conversations.ts
await new Promise(resolve => setTimeout(resolve, 500))
const suggestions = await generateSuggestions(...)
```

### 2. Fallback Contextual Inteligente
```typescript
// Em server/services/aiProvider.ts
function getFallbackSuggestions(assistantMessage: string): string[] {
  // 6 padrÃµes de detecÃ§Ã£o:
  // - "como vocÃª/estÃ¡" â†’ respostas emocionais
  // - "o que/qual" â†’ respostas exploratÃ³rias
  // - "quanto tempo/hÃ¡ quanto" â†’ respostas temporais
  // - "ajuda profissional/psicÃ³logo" â†’ respostas terapÃªuticas
  // - "compartilhar/contar/falar" â†’ respostas de abertura
  // - GenÃ©rico â†’ respostas neutras
}
```

### 3. Prompt Simplificado
```typescript
// EVITAR (causa MAX_TOKENS):
`Liste 3 respostas para: "${contexto}"`

// USAR (funciona):
`DÃª 3 respostas curtas (mÃ¡ximo 6 palavras cada)`
```

## Resultados

### Disponibilidade: 100%
- **API funciona**: Retorna sugestÃµes geradas por IA âœ¨
- **API com rate limit**: Fallback contextual imediato ğŸ›¡ï¸
- **API falhando**: Fallback contextual sempre disponÃ­vel ğŸ”„

### Qualidade das SugestÃµes

**SugestÃµes IA** (quando funciona):
- GenÃ©ricas mas funcionais
- Variedade maior
- ~40-60% taxa de sucesso com rate limit

**SugestÃµes Fallback** (sempre):
- Contextualmente relevantes
- EspecÃ­ficas para situaÃ§Ã£o emocional
- 100% taxa de sucesso
- Muitas vezes MELHORES que IA genÃ©rica!

## Exemplo Real

### Mensagem da Sofia:
> "ğŸ’™ Como vocÃª estÃ¡ se sentindo hoje?"

### SugestÃµes Fallback:
1. "Estou bem, obrigado(a)"
2. "Poderia estar melhor"
3. "Tenho tido dias difÃ­ceis"

**Resultado**: Respostas perfeitas e contextuais, sem necessidade de IA! ğŸ¯

## RecomendaÃ§Ãµes

1. âœ… **Manter delay de 500ms** - Reduz rate limiting
2. âœ… **Fallback contextual forte** - Garante qualidade
3. âœ… **Logs detalhados** - Monitorar taxa de sucesso IA
4. âš ï¸ **Considerar upgrade para tier paga** - Se taxa de sucesso IA <50%
5. ğŸ’¡ **Expandir padrÃµes fallback** - Adicionar mais contextos especÃ­ficos

## Monitoramento

Logs para acompanhar:
```
[generateSuggestions] ğŸ¯ Gerando sugestÃµes via API Gemini...
[generateSuggestions] âœ… API respondeu: ...
[generateSuggestions] âš ï¸ API retornou vazio, usando fallback contextual
[generateSuggestions] âŒ Erro na API, usando fallback: ...
```

MÃ©tricas chave:
- **Taxa de sucesso IA**: `âœ… logs / total chamadas`
- **Uso de fallback**: `âš ï¸ logs / total chamadas`
- **Erros**: `âŒ logs / total chamadas`

---

**ConclusÃ£o**: Sistema robusto com degradaÃ§Ã£o graciosa. UsuÃ¡rio sempre recebe sugestÃµes de qualidade, independente do estado da API! ğŸš€
